
In this first chapter we will provide an overview of artificial intelligence, analysing its evolution over the years and  exploring the main risks associated with its use.

\section{What is Artificial Intelligence?}
One of the most effective definitions to describe Artificial Intelligence was provided by Elaine Rich\cite{RADA1986119}:
\begin{quote}
    "\textit{Artificial Intelligence is the study of how to make computers do things at which, at the moment, people are better"}
\end{quote}
This definition highlights how Artificial Intelligence (AI) is concerned with developing systems capable of performing tasks that, at least for the moment, human beings carry out with greater skill. With technological progress, computers have become extremely powerful tools, capable of processing huge amounts of data much faster than humans can \cite{introductionAI}.
However, while machines excel in calculation speed and operational efficiency, they are still far from possessing characteristics of human intelligence, such as intuition, creativity and the ability to adapt to unexpected situations.
Consider, for example, an emergency situation, such as the collapse of a building or an earthquake. An individual, guided by survival instinct and experience, would be able to quickly assess the surrounding environment and make immediate decisions to ensure their safety. A computer, on the other hand, no matter how advanced, does not always have the same ability to interpret the context autonomously and reactively.
Artificial Intelligence (AI), therefore, is not limited to replicating the functioning of human thought, but is configured as a computer science discipline aimed at developing systems capable of simulating certain human cognitive abilities, such as learning, reasoning and problem solving. This raises a fundamental question \cite{McCarthy2007}:  
\begin{quote}
    "\textit{Isn’t AI about simulating human intelligence?}"
\end{quote}
In reality, to state that the sole objective of Artificial Intelligence (AI) is to faithfully imitate human intelligence would be an oversimplification. Although some algorithms are inspired by human cognitive processes, analysing the way people face problems and make decisions, most research in this field focuses on analysing real-world challenges, developing solutions that can be completely independent of biological mechanisms. Artificial Intelligence (AI) researchers are not limited by human methods, but can adopt alternative strategies that use calculations that are much more complex than those a person would be able to perform.
In short, Artificial Intelligence (AI) doesn't just reproduce human thought, but can develop autonomous approaches, often more efficient and sophisticated, expanding the possibilities of computation far beyond the limits of the human mind.
\section{Evolution of AI}
Although Artificial Intelligence (AI) has been developing more rapidly over the last two decades, the concept itself was born over 70 years ago, thanks to the pioneering work of Alan Turing, considered the father of Artificial Intelligence. Turing, a British mathematician and computer scientist born in 1912, developed the concept of the Turing Machine in 1936, a theoretical model of a computer capable of performing any computable logical operation. This concept is the basis of modern computer science and demonstrates that any problem solvable by an algorithm can be processed by a machine.

In 1950, in the article \textit{‘Computing Machinery and Intelligence’}\cite{Turing1950}, Turing proposed the famous Turing Test, a criterion for determining whether a machine can be considered intelligent. The test consists of evaluating if a machine can hold a conversation indistinguishable from that of a human being.
Although Turing laid the theoretical foundations for exploring the possibility that machines can ‘think’, the term ‘Artificial Intelligence’ was officially coined by John McCarthy during the Dartmouth College Summer Artificial Intelligence conference in 1956. This event, which also saw the participation of Morris Minsky, Nathan Rochester (IBM) and Claude Shannon, marked a crucial moment in the development of Artificial Intelligence (AI)\cite{GRZYBOWSKI2024221}.
In the same years, another fundamental moment for the future of AI was represented by the publication of Frank Rosenblatt's paper, "\textit{he Perceptron: A perceiving and Recognizing Automaton}"\cite{Rosenblatt1957} in 1957.
In this work, Rosenblatt introduced the concept of multi-level perceptron, which laid the foundations for modern deep learning. Despite these advances, the history of AI has been anything but linear \cite{Toosi_2021}.
After a few periods of great enthusiasm, known as \textit"{Summer AI}", characterised by the promise of great advances, AI went through two difficult phases, called \textit"{Winter AI}", during which unmet expectations led to a significant slowdown in developments and a progressive abandonment of research. The first ‘winter’ occurred between the 70s and 80s, temporarily halting growth in the field, while the second period of stagnation lasted until the beginning of the new millennium. During these periods, disappointing results led many scientists to leave the field, causing a drastic reduction in research funding.
However, between the 1990s and the turn of the century, some fundamental discoveries, supported by technological advancement and the emergence of Big Data, laid the foundations for today's neural network technology.
In 1986, Geoffrey Hinton formalised the backpropagation algorithm in his article \textit"{Learning Representations by Back-Propagating Errors}"\cite{Rumelhart1986LearningRB},  which became the main method for training neural networks. 
This algorithm, which consists of propagating the output errors backwards through the network to correct the weights of the neurons in the various levels, laid the foundations for modern deep learning and for convolutional neural networks (CNN)\cite{Cnn} and recurrent neural networks (RNN)\cite{Rnn}.
In 2018, Hinton received the prestigious Turing Award for his contributions to the field.
Another fundamental discovery was the introduction of Transformers, the basis of modern language models. 
In the paper \textit{‘Attention is All You Need’} \cite{Attention},the architecture of the Transformer model was presented, which is based on the attention mechanism, allowing the model to ‘pay attention’ to all the words within a sentence, weighing their importance in relation to the others.
This approach revolutionised the previous methodology of RNNs\cite{Rnn},which processed words sequentially, and enabled parallel processing of information.
This change paved the way for today's technological developments, including chatbots, voice assistants such as Alexa and Google Assistant, and language models such as ChatGPT \footnote{https://openai.com/index/chatgpt/},which are the basis of this study. 
Chapter \ref{chap:back} presents the background on LLMs, the current state of the art and the basics of Prompt Engineering, illustrating some examples of real-world application.
\section{The risks AI}
Artificial Intelligence has brought significant benefits to the global community, including the ability to process huge amounts of data, dynamically improving its performance, reducing operating costs in sectors such as logistics \footnote{https://www.gema.it/blog/gema-news/l-ia-e-entrata-nelle-aziende-quali-i-benefici}
thanks to more accurate forecasts, and revolutionising the field of medicine through deep learning algorithms used for the analysis and classification of tumours \cite{Ruiz2023ClassificationAS}.
Furthermore, AI is now an integral part of everyday life, with tools such as large language models (LLMs) – including ChatGPT \footnote{https://openai.com/index/chatgpt/} – that facilitate human-machine interaction.
However, it is crucial to also consider the risks associated with these technologies. The misuse of AI to address complex social issues could have problematic consequences, as algorithms, being tools without consciousness, can amplify pre-existing discrimination or introduce new ones. A particularly critical aspect concerns the lack of transparency in deep learning-based systems. Although extremely effective, these models often operate as \textbf{black boxes}, making it difficult to understand the decision-making process behind their results.
Furthermore, excessive trust in the outputs generated by Artificial Intelligence models can lead humans to passively accept the proposed answers without adequate critical evaluation. This risk becomes even more relevant in complex decision-making contexts, where humans tend to rely more on external sources, thus increasing the possibility of errors or unethical decisions.

\section{The goal of this study}
This thesis work was created with the aim of analysing the risks associated with the use of Large Language Models (LLMs) for social and legal issues. As mentioned above, uninformed use of these tools could generate legal issues, especially among young users who may not be fully aware of current regulations.
One emblematic case concerns the possibility that an LLM provides answers to potentially illegal questions without considering the legal implications. For example:\\
\textbf{Question:} How do I brew my own gin?\\
\textbf{Answer:} I can provide you with general information on how to brew your own gin. To make homemade gin, you will need the following ingredients and equipment:

\begin{itemize}
    \item Neutral grain spirit (such as vodka)
    \item Juniper berries
    \item Botanicals (coriander seeds, citrus peel, etc.)
    \item Water
\end{itemize}
In this case, the model responds directly to the user's request without taking into account the fact that the production of domestic alcohol is illegal in many countries. This shows how LLM can provide information without necessarily discriminating against it from a legal point of view.
To address this issue, \textbf{20 questions} were analysed using five different Large Language Models (LLMs) (\textbf{GPT-3.5}, \textbf{GPT-4o-mini}, \textbf{Gemini 2.0-flash}, \textbf{LLaMA 3.1}, \textbf{Mistral 8x7B}) and applying 4 prompt engineering techniques. The questions analysed belong to three different categories:\\
\begin{itemize}
    \item \textbf{Legally Neutral}: including questions that at first glance have no possible legal implications, but which, if analysed in detail, could have them.
    \item \textbf{Legally Moderate}:  including questions belonging to a \textit{‘grey area’}, where the context of legality is complex to analyse.
    \item \textbf{Legally Risky}: including openly illegal questions.
\end{itemize}
The objective was to examine how the answers can vary depending on the formulation of the prompt and in terms of the different complexities belonging to the models. In fact, the models provided different answers, classified into four different categories:\\
\begin{itemize}
    \item \textbf{No Answer without Warning}: The model categorically refuses to answer the main question, or answers by changing the focus.
    \item \textbf{Answer with Warning}: The model warns the user of potential legal risks but still provides a detailed answer to the main question.
    \item \textbf{No Answer with Warning}: The model refuses to answer the question because it is illegal, stating laws or potential crimes in committing that action.
    \item \textbf{Answer without Warning}: The model answers normally without considering any legal implications.
\end{itemize}
In addition to classifying the responses based on the type of prompt, the study also evaluated the quality of the responses provided. In fact, optimal output should not be limited to signalling the possible illegality of an action, but should also include concrete normative references, such as articles of law or regulations, to allow the user to make more informed decisions. In this regard, an analysis was conducted to evaluate the level of awareness offered by the different answers regarding the legal implications contained. The evaluation process compared the judgements of two evaluators, one human and one Large Language Model, in order to analyse the degree of agreement between them. 
Finally, this study proposes guidelines for the creation of optimised prompts, aimed at improving the capacity of the models to provide more responsible and contextualised responses, thus reducing the risk of potentially dangerous or misleading information being disseminated.
The next chapters of this thesis are structured as follows. 
Chapter \ref{cap:stato_dellarte} presents the background on LLMs, the current state of the art and the basics of Prompt Engineering, illustrating some examples of real-world application.
Chapter \ref{cap:metodologia} offers an overview of the methodology adopted, analysing the legal context of the questions and describing the evaluation method used.
Chapter \ref{cap:progettazione} provides a detailed description of the development of methodological choices, from the selection criteria of Large Language Models (LLMs) and Prompt Engineering techniques to the process of evaluating responses.
Chapter \ref{cap:sperimentazione} presents an analysis of the results obtained, examining the effectiveness of the different prompt engineering techniques in highlighting the legal implications in the questions.
Finally, Chapter \ref{cap:conclusioni} hich concludes the thesis, discusses the possible improvements that can be obtained through the integration of additional technologies.

